# Hydra 配置组合：决定了最终配置是如何由多个文件组合而成的。
# 这是一个"默认套餐"，程序会先加载这里的设置，然后可以用命令行参数覆盖它们。
defaults:
  - _self_                                    # 首先加载当前文件（config.yaml）中的所有设置。
  - dset: musdb44                             # 然后加载 `conf/dset/musdb44.yaml` 文件，用它的内容覆盖数据集（dset）相关的默认值。
  - svd: default                              # 加载 `conf/svd/default.yaml` 的 SVD 相关配置。
  - variant: default                          # 加载 `conf/variant/default.yaml` 的变体相关配置。
  - override hydra/hydra_logging: colorlog    # 强制覆盖 Hydra 的日志记录器，使用 colorlog 美化输出。
  - override hydra/job_logging: colorlog     # 同上。

dummy:                                          # 用于调试的虚拟/空部分。

# --- 数据集 (Dataset) 配置 ---
# 所有与训练/验证数据相关的设置都在这里。
dset:
  musdb: null                                  # 不使用MUSDB数据集
  musdb_samplerate: 44100                      # MUSDB 数据集的原始采样率。
  use_musdb: false                            # 不使用MUSDB数据集
  musdb_path: null                            # 明确设置MUSDB路径为null
  wav: E:/CODE/python_code/inst/demucs/data/instrumental          # 你的自定义WAV训练集路径
  wav2: false                                  # 第二个自定义 WAV 训练集的根目录路径。
  segment: 8                                   # 保持4秒分段，平衡性能和内存使用
  shift: 1                                     # 训练时，对每个片段做多少次随机移位（shift）的数据增强。
  train_valid: false                           # 是否从训练集中切分一部分作为验证集。
  full_cv: false                               # 是否使用完整的交叉验证数据。
  samplerate: 44100                           # 训练过程中统一使用的采样率 (会自动重采样)。
  channels: 2                                 # 训练过程中统一使用的音频通道数 (会自动转换)。
  normalize: true                             # 是否对音频进行标准化处理。
  metadata: ./metadata                        # 存储元数据的路径。
  sources: ['instrumental']        # 双轨配置：原曲和伴奏
  valid_samples:                              # 验证集使用的样本数量。
  backend: null                               # 指定 torchaudio 的后端，如 'soundfile'。

# --- 测试/评估 (Evaluation) 配置 ---
# 控制在测试集上进行模型评估时的行为。
test:
  save: true        # 在验证时保存分离出的音频文件，可以听到验证效果
  best: true        # 训练结束后，是否自动加载在验证集上表现最好的那个模型权重。
  workers: 1        # 评估时使用的数据加载进程数。
  every: 100          # 每训练5个epoch，进行一次完整的测试评估
  split: true       # 评估时是否启用 "Overlap" 和 "Shifts" 来获取更高质量的分离结果（但会更慢）。
  shifts: 1         # 评估时的随机移位次数。0 表示不启用，>0 会显著增加评估时间但提升分离质量。
  overlap: 0.25     # 评估时，音频片段之间重叠的比例。可以减少拼接痕迹（stitching artifacts）。
  sdr: false        # 禁用SDR计算，因为我们没有MUSDB测试集
  metric: 'loss'    # 在验证集上选择最佳模型时使用的指标。可以是 'loss' (损失) 或 'nsdr' (归一化 SDR)。
  nonhq: null       # 禁用MUSDB测试集评估

# --- 核心训练参数 ---
epochs: 2              # 训练总轮次：回到成功配置，给更多训练时间
batch_size: 1           # 批大小：保持不变
max_batches: null       # 限制每个epoch最多处理100个batch，快速测试

# --- 优化器 (Optimizer) 配置 ---
# 定义了如何根据损失来更新模型权重。
optim:
  lr: 4e-4           # 学习率
  momentum: 0.88       # 动量 (Momentum)：Adam 优化器的参数之一。
  beta2: 0.98        # Adam 优化器的参数之二。
  loss: l1            # 损失函数：'l1' (绝对值误差) 在音频领域常用，'mse' (均方误差) 是另一个选项。
  optim: adam         # 优化算法：Adam 是目前的主流选择。
  weight_decay: 1e-5  # 权重衰减：一种正则化手段，用于防止模型过拟合。
  clip_grad: 0        # 梯度裁剪阈值：防止梯度爆炸。0 表示不启用。

seed: 45              # 随机种子：固定这个值可以确保每次实验的结果都可以复现。
debug: false          # 调试模式开关。
valid_apply: true     # 是否在验证时应用模型。
flag:                 # 一个空的标志位，可用于在命令行中传递自定义标志。
save_every:           # 每隔多少个 epoch 保存一次模型，而不是只保存最好的。
weights: [1.0]   # 训练/验证损失中，每个音轨所占的权重。只关心instrumental

# --- 数据增强 (Data Augmentation) 配置 ---
# 在训练时通过各种变换"创造"新数据，是提升模型泛化能力的关键。
augment:
  shift_same: true   # 对同一首歌的不同音轨，是否应用完全相同的随机时移。
  repitch:            # 变调/变速增强。
    proba: 0        # 恢复repitch增强
    max_tempo: 12     # 节拍变化的最大范围。
  remix:              # 音轨混合增强：将不同歌曲的音轨混合在一起，创造新的训练样本。
    proba: 0          # 禁用remix增强，因为我们是单轨分离
    group_size: 4     # 每次从 4 首不同的歌里随机抽取音轨来混合。
  scale:              # 音量缩放增强。
    proba: 0.8        # 90% 的概率应用此增强。
    min: 0.15          # 最小缩放到原音量的 0.5 倍。
    max: 1.75          # 最大放大到原音量的 1.5 倍。
  flip: true          # 随机进行左右声道翻转增强。

# --- 继续训练 / 微调 (Finetuning) 配置 ---
continue_from: null         # 从一个完整的 Dora 实验记录（签名）中继续训练。
continue_pretrained: null    # 【微调关键】从一个预训练模型的签名继续训练（例如 'htdemucs_ft'）。这不会加载优化器状态。
pretrained_repo: null        # 预训练模型所在的仓库路径或 URL。
continue_best: true     # 加载时，是加载验证集上表现最好的模型权重 (True)，还是最后一次训练的权重 (False)。
continue_opt: false     # 是否同时加载优化器的状态。对于微调，通常设为 False。

# --- 其他杂项配置 ---
misc:
  num_workers: 0      # 数据加载时使用的 CPU 进程数，设为0避免多进程问题
  num_prints: 120       # 每个 epoch 打印多少次训练信息，减少到2
  show: false         # 是否显示图谱等可视化信息（需要图形界面）。
  verbose: false      # 是否打印更详细的日志。

# --- 指数移动平均 (Exponential Moving Average) 配置 ---
# EMA 是一种通过平均模型在训练过程中的权重来提升最终性能和稳定性的技巧。
# Batch 级别的 EMA 在 GPU 上计算以保证速度。
ema:
  epoch: []        # 在 epoch 级别应用的 EMA 衰减率列表，回到成功配置。
  batch: [0.9992,0.9997]       # 在 batch 级别应用的 EMA 衰减率列表，回到成功配置。

use_train_segment: true  # (待移除的旧参数)
model_segment: 16        # 覆盖模型的分段参数，4秒 * 4 = 16秒
model: htdemucs_p        # 使用改进版HTDemucs模型

# --- Hybrid Transformer Demucs (HTDemucs) 模型配置 (当前最先进的模型之一) ---
htdemucs_p:
  # --- 卷积部分 (U-Net) ---
  channels: 32          # 回到成功配置的通道数
  channels_time:        # 时域分支通道数。
  growth: 2             # 通道增长率。
  # STFT
  nfft: 4096            # STFT 窗口大小。
  wiener_iters: 0       # 维纳滤波迭代次数。
  end_iters: 0          # 结束时额外迭代次数。
  wiener_residual: false# 对残差进行维纳滤波。
  cac: true             # Complex as Channels 技术。
  # Main structure
  depth: 6              # 减少U-Net深度
  rewrite: true         # 使用重写的高效实现。
  # Frequency Branch
  multi_freqs: []       # 多频率分辨率列表。
  multi_freqs_depth: 3  # 多分辨率分支深度。
  freq_emb: 0.2         # 频率嵌入比例。
  emb_scale: 10         # 嵌入缩放系数。
  emb_smooth: true      # 平滑嵌入。
  # Convolutions
  kernel_size: 8        # 卷积核大小。
  stride: 4             # 卷积步长。
  time_stride: 4        # 时域步长。
  context: 1            # 卷积上下文。
  context_enc: 0        # 编码器上下文。
  # normalization
  norm_starts: 4        # 开始归一化的层。
  norm_groups: 4        # 归一化组数。
  # DConv residual branch
  dconv_mode: 1         # DConv 模式。
  dconv_depth: 2        # DConv 深度。
  dconv_comp: 8         # 回到成功配置的DConv压缩率
  dconv_init: 1e-3      # DConv 初始化系数。
  # Before the Transformer
  bottom_channels: 0    # Transformer 前置层的通道数。0 表示与上一层相同。
  # --- 跨流 Transformer (Cross-Transformer) 部分 ---
  # ------ Common to all (通用参数) ------
  t_layers: 4           # 减少Transformer层数
  t_hidden_scale: 4.0   # Transformer 中前馈网络（FFN）的隐藏层维度放大系数。
  t_heads: 8            # 减少注意力头数
  t_dropout: 0       # 增加dropout防止过拟合
  t_layer_scale: True   # 是否使用 LayerScale 技术来稳定训练。
  t_gelu: True          # 是否使用 GELU 激活函数。
  # ------------- Positional Embedding (位置编码) -------------
  t_emb: sin            # 位置编码的方式, 'sin' 表示经典的 sin/cos 编码。
  t_max_positions: 10000 # 缩放位置编码的最大位置。
  t_max_period: 10000.0 # sin/cos 编码的最大周期。
  t_weight_pos_embed: 1.0 # 位置编码的权重。
  t_cape_mean_normalize: True # 是否对 CAPE (Conditional Absolute Position Embeddings) 进行均值归一化。
  t_cape_augment: True      # 是否对 CAPE 进行数据增强。
  t_cape_glob_loc_scale: [5000.0, 1.0, 1.4] # CAPE 的全局/局部缩放参数。
  t_sin_random_shift: 0     # 对 sin 位置编码进行随机移位的数量。
  # ------------- norm before a transformer encoder (Encoder 前的归一化) -------------
  t_norm_in: True       # 是否在 Transformer Encoder 输入前进行归一化。
  t_norm_in_group: False# 是否使用 Group Norm。
  # ------------- norm inside the encoder (Encoder 内部的归一化) -------------
  t_group_norm: False   # 是否使用 Group Norm。
  t_norm_first: True    # 是否使用 Pre-Norm 结构 (Norm 在 Attention/FFN 之前)，有助于稳定训练。
  t_norm_out: True      # 是否在 Transformer Encoder 输出后进行归一化。
  # ------------- optim (优化相关) -------------
  t_weight_decay: 0.0   # Transformer 参数的权重衰减。
  t_lr:                 # 为 Transformer 参数设置独立的学习率 (留空则使用全局学习率)。
  # ------------- sparsity (稀疏注意力，用于提升计算效率) -------------
  t_sparse_self_attn: False   # 是否在自注意力中使用稀疏注意力。
  t_sparse_cross_attn: False  # 是否在交叉注意力中使用稀疏注意力。
  t_mask_type: diag           # 稀疏掩码的类型。
  t_mask_random_seed: 42      # 稀疏掩码的随机种子。
  t_sparse_attn_window: 400   # 稀疏注意力的窗口大小。
  t_global_window: 100        # 全局注意力的窗口大小。
  t_sparsity: 0.95            # 注意力矩阵的稀疏度。
  t_auto_sparsity: False      # 是否自动调整稀疏度。
  # Cross Encoder First (False)
  t_cross_first: False        # 是否先进行交叉注意力再进行自注意力。
  # Weight init
  rescale: 0.1                # 权重初始化缩放因子。
  # Metadata
  use_train_segment: true     # 使用训练时的segment长度

# --- 奇异值分解 (Singular Value Decomposition) 正则化 ---
svd:
  penalty: 1e-5        # SVD 惩罚项的权重。
  min_size: 0.1     # 应用 SVD 的最小张量大小。
  dim: 1            # SVD 的维度。
  niters: 2         # SVD 幂迭代的次数。
  powm: false       # 是否使用幂方法。
  proba: 1          # 应用 SVD 的概率。
  conv_only: false  # 是否只对卷积层应用 SVD。
  convtr: false     # 是否对转置卷积层应用 SVD。
  bs: 1             # SVD 的批大小。

# --- 模型量化 (Quantization) 配置 ---
quant:
  diffq:            # DiffQ (Differentiable Quantization) 惩罚项, 通常是 1e-4 或 3e-4。
  qat:              # QAT (Quantization-Aware Training) 使用的比特数。
  min_size: 0.2     # 应用量化的最小张量大小。
  group_size: 8     # 量化的组大小。

# --- Dora 实验管理工具配置 ---
dora:
  dir: outputs      # 所有实验结果（日志、模型权重、配置文件快照）的输出目录。
  exclude: ["misc.*", "slurm.*", 'test.reval', 'flag', 'dset.backend'] # 保存配置快照时，排除掉一些不需要保存的项。

# --- SLURM 集群作业调度系统配置 (个人用户可以完全忽略) ---
slurm:
  time: 4320
  constraint: volta32gb
  setup: ['module load cudnn/v8.4.1.50-cuda.11.6 NCCL/2.11.4-6-cuda.11.6 cuda/11.6']

# --- Hydra 框架自身配置 ---
hydra:
  job_logging:
    formatters:
      colorlog:
        datefmt: "%m-%d %H:%M:%S" # 定义日志中日期的格式。